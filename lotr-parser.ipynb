{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time\n",
    "from Util import Env, quantizer, shifter, get_parser\n",
    "datadir = 'lotr/04-crypto-sc/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onefiles = os.listdir(datadir + 'eddsa-1-parsed')\n",
    "onelist = []\n",
    "for fname in onefiles:\n",
    "    with open(datadir + 'eddsa-1-parsed/' + fname, 'r') as f:\n",
    "        onelist.append([int(n) for n in f])\n",
    "print(len(onelist))\n",
    "onethreshold = np.percentile(np.array([len(l) for l in onelist]),0.3)\n",
    "\n",
    "zerofiles = os.listdir(datadir + 'eddsa-0-parsed')\n",
    "zerolist = []\n",
    "for fname in zerofiles:\n",
    "    with open(datadir + 'eddsa-0-parsed/' + fname, 'r') as f:\n",
    "        zerolist.append([int(n) for n in f])\n",
    "print(len(zerolist))\n",
    "zerothreshold = np.percentile(np.array([len(l) for l in zerolist]),0.3)\n",
    "print((onethreshold, zerothreshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(min(zerothreshold, onethreshold))\n",
    "newzero = [z for z in zerolist if len(z) >= threshold]\n",
    "newone = [o for o in onelist if len(o) >= threshold]\n",
    "dsize = min((len(newzero),len(newone)))\n",
    "x_arr = np.zeros([dsize*2, threshold],dtype=np.float32)\n",
    "y_arr = np.arange(dsize*2)%2\n",
    "for i in range(dsize):\n",
    "    x_arr[2*i] = newzero[i][:threshold]\n",
    "    x_arr[2*i+1] = newone[i][:threshold]\n",
    "print(x_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 16\n",
    "mean = np.median(x_arr)\n",
    "x_arr2 = (x_arr-mean)/std\n",
    "print((mean,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_arr2[:2000]\n",
    "train_y = y_arr[:2000]\n",
    "valid_x = x_arr2[2000:4000]\n",
    "valid_y = y_arr[2000:4000]\n",
    "test_x = x_arr2[2000:5000]\n",
    "test_y = y_arr[2000:5000]\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel = 'rbf',gamma=0.1, C=0.5)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"eddsa.pkl\"):\n",
    "    x_arr_old, y_arr_old = pickle.load(open(\"eddsa.pkl\",'rb'))\n",
    "    newlen = min(x_arr_old.shape[1], x_arr.shape[1])\n",
    "    print(\"Old tracelen {}, new tracelen {}\".format(x_arr_old.shape[1], newlen))\n",
    "    x_arr_new = np.concatenate([x_arr_old[:,:newlen], x_arr[:,:newlen]], axis=0)\n",
    "    y_arr_new = np.concatenate([y_arr_old, y_arr])\n",
    "else:\n",
    "    x_arr_new = x_arr\n",
    "    y_arr_new = y_arr\n",
    "print(\"total num traces: \" + str(len(y_arr_new)))\n",
    "traincut = 7*(len(y_arr_new)//10)\n",
    "valcut = 17*(len(y_arr_new)//20)\n",
    "pickle.dump((x_arr_new, y_arr_new), open(\"eddsa.pkl\", \"wb\"))\n",
    "pickle.dump((x_arr_new[:traincut], y_arr_new[:traincut]), open(\"eddsa_train.pkl\", \"wb\"))\n",
    "pickle.dump((x_arr_new[traincut:valcut], y_arr_new[traincut:valcut]), open(\"eddsa_valid.pkl\", \"wb\"))\n",
    "pickle.dump((x_arr_new[valcut:], y_arr_new[valcut:]), open(\"eddsa_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onefiles = os.listdir(datadir + 'rsa-1-parsed')\n",
    "onelist = []\n",
    "for fname in onefiles:\n",
    "    with open(datadir + 'rsa-1-parsed/' + fname, 'r') as f:\n",
    "        onelist.append([int(n) for n in f])\n",
    "print(len(onelist))\n",
    "onethreshold = np.percentile(np.array([len(l) for l in onelist]),0.3)\n",
    "\n",
    "zerofiles = os.listdir(datadir + 'rsa-0-parsed')\n",
    "zerolist = []\n",
    "for fname in zerofiles:\n",
    "    with open(datadir + 'rsa-0-parsed/' + fname, 'r') as f:\n",
    "        zerolist.append([int(n) for n in f])\n",
    "print(len(zerolist))\n",
    "zerothreshold = np.percentile(np.array([len(l) for l in zerolist]),0.3)\n",
    "print((onethreshold, zerothreshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(min(zerothreshold, onethreshold))\n",
    "newzero = [z for z in zerolist if len(z) >= threshold]\n",
    "newone = [o for o in onelist if len(o) >= threshold]\n",
    "dsize = min((len(newzero),len(newone)))\n",
    "x_arr = np.zeros([dsize*2, threshold],dtype=np.float32)\n",
    "y_arr = np.arange(dsize*2)%2\n",
    "for i in range(dsize):\n",
    "    x_arr[2*i] = newzero[i][:threshold]\n",
    "    x_arr[2*i+1] = newone[i][:threshold]\n",
    "print(x_arr.shape)\n",
    "\n",
    "std = 16\n",
    "mean = np.median(x_arr)\n",
    "x_arr2 = (x_arr-mean)/std\n",
    "print((mean,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_arr2[:7000]\n",
    "train_y = y_arr[:7000]\n",
    "valid_x = x_arr2[7000:8500]\n",
    "valid_y = y_arr[7000:8500]\n",
    "test_x = x_arr2[8500:10000]\n",
    "test_y = y_arr[8500:10000]\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel = 'rbf',gamma=0.01, C=10.0)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((x_arr, y_arr), open(\"rsa.pkl\", \"wb\"))\n",
    "pickle.dump((x_arr[:7000], y_arr[:7000]), open(\"rsa_train.pkl\", \"wb\"))\n",
    "pickle.dump((x_arr[7000:8500], y_arr[7000:8500]), open(\"rsa_valid.pkl\", \"wb\"))\n",
    "pickle.dump((x_arr[8500:10000], y_arr[8500:10000]), open(\"rsa_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2657186c0a85c2e6baa4c072599c792136a86a2a4c2021b8710be0ae7329b2e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
