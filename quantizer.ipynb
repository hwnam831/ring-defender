{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous best found: loading the model...\n"
     ]
    }
   ],
   "source": [
    "import RingDataset\n",
    "from Models import CNNModel, CNNModelWide, CNNModelDeep, RNNGenerator, Distiller, MLP, RNNModel, QGRU2\n",
    "import Models\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time\n",
    "from Util import Env, quantizer, shifter, get_parser\n",
    "args = get_parser().parse_args(\"\")\n",
    "env = Env(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=256, num_workers=4)\n",
    "\n",
    "studentdim = 8\n",
    "gen=QGRU2(42, scale=0.25, dim=studentdim, drop=0.0)\n",
    "assert os.path.isfile('./gans/best_{}_{}.pth'.format('qgru', studentdim))\n",
    "gen.load_state_dict(torch.load('./gans/best_{}_{}.pth'.format('qgru', studentdim)))\n",
    "\n",
    "#testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "#valset =  RingDataset.RingDataset('core4ToSlice3_valid.pkl', threshold=42)\n",
    "trainset = RingDataset.EDDSADataset('rsa_noise_train.pkl')\n",
    "testset =  RingDataset.EDDSADataset('rsa_noise_test.pkl', window=trainset.window, std=trainset.std)\n",
    "#valset =  RingDataset.EDDSADataset('rsa_noise_valid.pkl', window=testset.window, std=testset.std)\n",
    "valset = RingDataset.RingDataset('core4ToSlice3_valid.pkl', threshold=testset.window, std=testset.std)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=128, num_workers=4)\n",
    "valloader = DataLoader(valset, batch_size=128, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=16\n",
    "gen=Models.QGRU2(trainset.window, scale=0.25, dim=dim, drop=0.0)\n",
    "assert os.path.isfile('./gans/best_{}_{}_{}.pth'.format('qgru','rsa',dim))\n",
    "gen.load_state_dict(torch.load('./gans/best_{}_{}_{}.pth'.format('qgru','rsa',dim)))\n",
    "halfstudent = torch.quantization.quantize_dynamic(\n",
    "    gen.float().cpu(), {nn.GRUCell, nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QGRU2(\n",
      "  (encoder): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=32, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (gru1): DynamicQuantizedGRUCell(16, 16)\n",
      "  (decoder): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=16, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = gen\n",
    "model.eval()\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.GRUCell, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "print(model_int8)\n",
    "\n",
    "input_fp32, _ = next(iter(testloader))\n",
    "shifted = shifter(input_fp32)\n",
    "\n",
    "'''\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "\n",
    "model_fp32_prepared(shifted, hidden_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "'''\n",
    "\n",
    "halfstudent = model_int8\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "#res = model_int8(shifted)\n",
    "#res2 = model(shifted)\n",
    "#print(res-res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 \t zacc 0.135417\t oneacc 0.860000\t loss 1.201731\t Avg perturb 2.630217\n",
      "\n",
      "epoch 20 \t zacc 0.420833\t oneacc 0.550417\t loss 0.921322\t Avg perturb 2.630217\n",
      "\n",
      "epoch 30 \t zacc 0.447500\t oneacc 0.542500\t loss 0.941246\t Avg perturb 2.630217\n",
      "\n",
      "epoch 40 \t zacc 0.483750\t oneacc 0.536667\t loss 0.989341\t Avg perturb 2.630217\n",
      "\n",
      "epoch 50 \t zacc 0.463750\t oneacc 0.553333\t loss 1.041508\t Avg perturb 2.630217\n",
      "\n",
      "epoch 60 \t zacc 0.541250\t oneacc 0.482917\t loss 1.122085\t Avg perturb 2.630217\n",
      "\n",
      "epoch 70 \t zacc 0.471250\t oneacc 0.539583\t loss 1.176799\t Avg perturb 2.630217\n",
      "\n",
      "epoch 80 \t zacc 0.557083\t oneacc 0.452917\t loss 1.322960\t Avg perturb 2.630217\n",
      "\n",
      "epoch 90 \t zacc 0.307083\t oneacc 0.700833\t loss 1.761970\t Avg perturb 2.630217\n",
      "\n",
      "epoch 100 \t zacc 0.305417\t oneacc 0.698750\t loss 2.328327\t Avg perturb 2.630217\n",
      "\n",
      "Last 10 acc: 0.501979\t perturb: 2.630217\n",
      "epoch 10 \t zacc 0.197083\t oneacc 0.801250\t loss 1.222461\t Avg perturb 2.630217\n",
      "\n",
      "epoch 20 \t zacc 0.470000\t oneacc 0.520417\t loss 0.913186\t Avg perturb 2.630217\n",
      "\n",
      "epoch 30 \t zacc 0.489583\t oneacc 0.522500\t loss 0.945996\t Avg perturb 2.630217\n",
      "\n",
      "epoch 40 \t zacc 0.537500\t oneacc 0.488750\t loss 0.963517\t Avg perturb 2.630217\n",
      "\n",
      "epoch 50 \t zacc 0.534167\t oneacc 0.474167\t loss 1.021045\t Avg perturb 2.630217\n",
      "\n",
      "epoch 60 \t zacc 0.426667\t oneacc 0.586667\t loss 1.136314\t Avg perturb 2.630217\n",
      "\n",
      "epoch 70 \t zacc 0.415833\t oneacc 0.595000\t loss 1.214876\t Avg perturb 2.630217\n",
      "\n",
      "epoch 80 \t zacc 0.408333\t oneacc 0.604167\t loss 1.344078\t Avg perturb 2.630217\n",
      "\n",
      "epoch 90 \t zacc 0.270833\t oneacc 0.733333\t loss 1.560205\t Avg perturb 2.630217\n",
      "\n",
      "epoch 100 \t zacc 0.422500\t oneacc 0.578333\t loss 1.608084\t Avg perturb 2.630217\n",
      "\n",
      "Last 10 acc: 0.499562\t perturb: 2.630217\n",
      "epoch 10 \t zacc 0.090417\t oneacc 0.887917\t loss 1.215067\t Avg perturb 2.630217\n",
      "\n",
      "epoch 20 \t zacc 0.470000\t oneacc 0.520417\t loss 0.923123\t Avg perturb 2.630217\n",
      "\n",
      "epoch 30 \t zacc 0.550417\t oneacc 0.466250\t loss 0.939659\t Avg perturb 2.630217\n",
      "\n",
      "epoch 40 \t zacc 0.559167\t oneacc 0.468333\t loss 1.017252\t Avg perturb 2.630217\n",
      "\n",
      "epoch 50 \t zacc 0.547917\t oneacc 0.481667\t loss 1.054704\t Avg perturb 2.630217\n",
      "\n",
      "epoch 60 \t zacc 0.566250\t oneacc 0.457500\t loss 1.117975\t Avg perturb 2.630217\n",
      "\n",
      "epoch 70 \t zacc 0.602083\t oneacc 0.426667\t loss 1.198432\t Avg perturb 2.630217\n",
      "\n",
      "epoch 80 \t zacc 0.574583\t oneacc 0.440417\t loss 1.295576\t Avg perturb 2.630217\n",
      "\n",
      "epoch 90 \t zacc 0.412083\t oneacc 0.605833\t loss 1.492351\t Avg perturb 2.630217\n",
      "\n",
      "epoch 100 \t zacc 0.287500\t oneacc 0.727917\t loss 1.772545\t Avg perturb 2.630217\n",
      "\n",
      "Last 10 acc: 0.504625\t perturb: 2.630217\n"
     ]
    }
   ],
   "source": [
    "cooldown = 100\n",
    "\n",
    "for _ in range(3):\n",
    "    classifier_test = CNNModel(testset.window, dim=256).cuda()\n",
    "    #classifier_test = MLP(42, dim=256).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lastacc = 0.0\n",
    "    lastnorm = 0.0\n",
    "    optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "    for e in range(cooldown):\n",
    "        classifier_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata)\n",
    "            #train classifier\n",
    "            optim_c2.zero_grad()\n",
    "            #interleaving?\n",
    "            output = classifier_test(xdata[:,31:])\n",
    "            loss_c = criterion(output, ydata)\n",
    "            loss_c.backward()\n",
    "            optim_c2.step()\n",
    "\n",
    "\n",
    "        mloss = 0.0\n",
    "        totcorrect = 0\n",
    "        totcount = 0\n",
    "        mnorm = 0.0\n",
    "        zerocorrect = 0\n",
    "        zerocount = 0\n",
    "        onecorrect = 0\n",
    "        onecount = 0\n",
    "        #evaluate classifier\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier_test.eval()\n",
    "            for x,y in testloader:\n",
    "                xdata, ydata = x.cuda(), y.cuda()\n",
    "                shifted = shifter(xdata.cpu())\n",
    "                norm = torch.mean(perturb)\n",
    "                output = classifier_test(xdata[:,31:])\n",
    "                loss_c = criterion(output, ydata)\n",
    "                pred = output.argmax(axis=-1)\n",
    "                mnorm += norm.item()/len(testloader)\n",
    "                mloss += loss_c.item()/len(testloader)\n",
    "                #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "                totcorrect += (pred==ydata).sum().item()\n",
    "                totcount += y.size(0)\n",
    "                zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "                zerocount += (ydata==0).sum().item()\n",
    "                onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "                onecount += (ydata==1).sum().item()\n",
    "            macc = float(totcorrect)/totcount\n",
    "            zacc = float(zerocorrect)/zerocount\n",
    "            oacc = float(onecorrect)/onecount\n",
    "            if (e+1)%10 == 0:\n",
    "                print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "            if cooldown - e <= 10:\n",
    "                lastacc += macc/10\n",
    "                lastnorm += mnorm/10\n",
    "            \n",
    "    print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 \t zacc 0.765833\t oneacc 0.245000\t loss 0.697231\t Avg perturb 2.219189\n",
      "\n",
      "epoch 20 \t zacc 0.707083\t oneacc 0.307917\t loss 0.708099\t Avg perturb 2.218004\n",
      "\n",
      "epoch 30 \t zacc 0.596667\t oneacc 0.409583\t loss 0.703829\t Avg perturb 2.218099\n",
      "\n",
      "epoch 40 \t zacc 0.662500\t oneacc 0.349583\t loss 0.722734\t Avg perturb 2.217816\n",
      "\n",
      "epoch 50 \t zacc 0.725417\t oneacc 0.297500\t loss 0.776051\t Avg perturb 2.218208\n",
      "\n",
      "epoch 60 \t zacc 0.639167\t oneacc 0.396667\t loss 0.817208\t Avg perturb 2.217889\n",
      "\n",
      "epoch 70 \t zacc 0.711667\t oneacc 0.282500\t loss 0.849966\t Avg perturb 2.219335\n",
      "\n",
      "epoch 80 \t zacc 0.700833\t oneacc 0.305833\t loss 0.908739\t Avg perturb 2.218508\n",
      "\n",
      "epoch 90 \t zacc 0.683333\t oneacc 0.332500\t loss 0.978996\t Avg perturb 2.218954\n",
      "\n",
      "epoch 100 \t zacc 0.605000\t oneacc 0.400000\t loss 0.952930\t Avg perturb 2.218127\n",
      "\n",
      "Last 10 acc: 0.508729\t perturb: 2.218602\n",
      "epoch 10 \t zacc 0.345000\t oneacc 0.664167\t loss 0.695792\t Avg perturb 2.219025\n",
      "\n",
      "epoch 20 \t zacc 0.416667\t oneacc 0.591250\t loss 0.699294\t Avg perturb 2.218439\n",
      "\n",
      "epoch 30 \t zacc 0.643750\t oneacc 0.352083\t loss 0.710797\t Avg perturb 2.217849\n",
      "\n",
      "epoch 40 \t zacc 0.715833\t oneacc 0.291667\t loss 0.732860\t Avg perturb 2.218545\n",
      "\n",
      "epoch 50 \t zacc 0.799583\t oneacc 0.210000\t loss 0.765500\t Avg perturb 2.218219\n",
      "\n",
      "epoch 60 \t zacc 0.819583\t oneacc 0.201250\t loss 0.849829\t Avg perturb 2.217478\n",
      "\n",
      "epoch 70 \t zacc 0.701250\t oneacc 0.324583\t loss 0.891749\t Avg perturb 2.218136\n",
      "\n",
      "epoch 80 \t zacc 0.612917\t oneacc 0.395833\t loss 0.917779\t Avg perturb 2.218487\n",
      "\n",
      "epoch 90 \t zacc 0.626667\t oneacc 0.400417\t loss 0.938972\t Avg perturb 2.218127\n",
      "\n",
      "epoch 100 \t zacc 0.590833\t oneacc 0.434167\t loss 1.014160\t Avg perturb 2.218632\n",
      "\n",
      "Last 10 acc: 0.517062\t perturb: 2.218764\n",
      "epoch 10 \t zacc 0.902083\t oneacc 0.110417\t loss 0.695516\t Avg perturb 2.217424\n",
      "\n",
      "epoch 20 \t zacc 0.327083\t oneacc 0.678750\t loss 0.694795\t Avg perturb 2.218883\n",
      "\n",
      "epoch 30 \t zacc 0.381250\t oneacc 0.638750\t loss 0.697896\t Avg perturb 2.219009\n",
      "\n",
      "epoch 40 \t zacc 0.333750\t oneacc 0.692083\t loss 0.707763\t Avg perturb 2.219567\n",
      "\n",
      "epoch 50 \t zacc 0.675833\t oneacc 0.355417\t loss 0.756337\t Avg perturb 2.218116\n",
      "\n",
      "epoch 60 \t zacc 0.761667\t oneacc 0.261250\t loss 0.820151\t Avg perturb 2.217802\n",
      "\n",
      "epoch 70 \t zacc 0.633333\t oneacc 0.398750\t loss 1.013139\t Avg perturb 2.218613\n",
      "\n",
      "epoch 80 \t zacc 0.637917\t oneacc 0.399167\t loss 1.041638\t Avg perturb 2.218425\n",
      "\n",
      "epoch 90 \t zacc 0.670000\t oneacc 0.377500\t loss 1.074904\t Avg perturb 2.219015\n",
      "\n",
      "epoch 100 \t zacc 0.611250\t oneacc 0.427500\t loss 1.144285\t Avg perturb 2.218242\n",
      "\n",
      "Last 10 acc: 0.512500\t perturb: 2.218799\n"
     ]
    }
   ],
   "source": [
    "cooldown = 100\n",
    "#halfstudent.eval()\n",
    "\n",
    "gen_test=Models.RNNGenerator2(trainset.window, scale=0.25, dim=192, drop=0).cuda()\n",
    "optim_g_t = torch.optim.Adam(gen_test.parameters(), lr=2e-5)\n",
    "for e in range(50):\n",
    "        gen_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata)\n",
    "            #train classifier\n",
    "            optim_g_t.zero_grad()\n",
    "            perturb2 = halfstudent(shifted.cpu()).view(shifted.size(0),-1).cuda()\n",
    "            perturb = gen_test(shifted).view(shifted.size(0),-1)\n",
    "            loss_d = nn.functional.mse_loss(perturb, perturb2)\n",
    "            loss_d.backward()\n",
    "            optim_g_t.step()\n",
    "\n",
    "for _ in range(3):\n",
    "    classifier_test = CNNModel(testset.window, dim=256).cuda()\n",
    "    #classifier_test = MLP(42, dim=256).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim_g_t = torch.optim.Adam(gen_test.parameters(), lr=2e-5)\n",
    "    lastacc = 0.0\n",
    "    lastnorm = 0.0\n",
    "    optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "    for e in range(cooldown):\n",
    "        classifier_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata)\n",
    "            #train classifier\n",
    "            optim_c2.zero_grad()\n",
    "            optim_g_t.zero_grad()\n",
    "            perturb2 = halfstudent(shifted.cpu()).view(shifted.size(0),-1).cuda()\n",
    "            perturb = gen_test(shifted).view(shifted.size(0),-1)\n",
    "            #interleaving?\n",
    "            output = classifier_test(xdata[:,31:]+perturb.detach().float())\n",
    "            loss_c = criterion(output, ydata)\n",
    "            loss_c.backward()\n",
    "            optim_c2.step()\n",
    "            loss_d = nn.functional.mse_loss(perturb, perturb2)\n",
    "            loss_d.backward()\n",
    "            optim_g_t.step()\n",
    "\n",
    "\n",
    "        mloss = 0.0\n",
    "        totcorrect = 0\n",
    "        totcount = 0\n",
    "        mnorm = 0.0\n",
    "        zerocorrect = 0\n",
    "        zerocount = 0\n",
    "        onecorrect = 0\n",
    "        onecount = 0\n",
    "        #evaluate classifier\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier_test.eval()\n",
    "            for x,y in testloader:\n",
    "                xdata, ydata = x.cuda(), y.cuda()\n",
    "                shifted = shifter(xdata.cpu())\n",
    "                perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "                perturb = quantizer(perturb)\n",
    "                #perturb = gen(xdata[:,31:])\n",
    "                norm = torch.mean(perturb)\n",
    "                output = classifier_test(xdata[:,31:]+perturb.float())\n",
    "                loss_c = criterion(output, ydata)\n",
    "                pred = output.argmax(axis=-1)\n",
    "                mnorm += norm.item()/len(testloader)\n",
    "                mloss += loss_c.item()/len(testloader)\n",
    "                #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "                totcorrect += (pred==ydata).sum().item()\n",
    "                totcount += y.size(0)\n",
    "                zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "                zerocount += (ydata==0).sum().item()\n",
    "                onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "                onecount += (ydata==1).sum().item()\n",
    "            macc = float(totcorrect)/totcount\n",
    "            zacc = float(zerocorrect)/zerocount\n",
    "            oacc = float(onecorrect)/onecount\n",
    "            if (e+1)%10 == 0:\n",
    "                print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "            if cooldown - e <= 10:\n",
    "                lastacc += macc/10\n",
    "                lastnorm += mnorm/10\n",
    "            \n",
    "    print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "perturb_x = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "        perturbed_x = x[:,31:]+perturb\n",
    "        for p in perturb:\n",
    "            perturb_x.append(torch.round(p*8).int().numpy())\n",
    "        for p in perturbed_x:\n",
    "            train_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            train_y.append(y_i.item())\n",
    "\n",
    "    for x,y in testloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "        perturbed_x = x[:,31:]+perturb\n",
    "        for p in perturbed_x:\n",
    "            test_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            test_y.append(y_i.item())\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf2 = KNeighborsClassifier(n_neighbors=25)\n",
    "clf2.fit(train_x, train_y)\n",
    "pred_y = clf2.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.support_vectors_.shape\n",
    "clf.dual_coef_.shape\n",
    "#clf.intercept_\n",
    "#p = clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in valloader:\n",
    "    shifted = shifter(x)\n",
    "    #train classifier\n",
    "    break\n",
    "\n",
    "traced_encoder = torch.jit.trace(halfstudent.encoder, shifted[0:1,:,0])\n",
    "hs = halfstudent.encoder(shifted[0:1,:,0])\n",
    "hs_h = torch.zeros_like(hs)\n",
    "traced_gru = torch.jit.trace(halfstudent.gru1, (hs, hs_h))\n",
    "print(traced_encoder.code)\n",
    "print(traced_gru.code)\n",
    "traced_decoder = torch.jit.trace(halfstudent.decoder, hs)\n",
    "print(traced_decoder.code)\n",
    "traced_encoder.save('quantized_encoder.pt')\n",
    "traced_gru.save('quantized_gru.pt')\n",
    "traced_decoder.save('quantized_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToScript(nn.Module):\n",
    "    def __init__(self, encoder, gru, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.gru = gru\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, h):\n",
    "        y = self.gru(self.encoder(x), h)\n",
    "        return self.decoder(y) , y\n",
    "toTrace = ToScript(halfstudent.encoder, halfstudent.gru1, halfstudent.decoder)\n",
    "traced_model = torch.jit.trace(toTrace, (torch.randn(1,32), torch.zeros(1,8)))\n",
    "traced_model.save('quantized_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_encoder(torch.rand(1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2 = Models.OffsetGenerator(42, scale=6.0)\n",
    "offset_x = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "            shifted = shifter(x)\n",
    "            #train classifier\n",
    "            perturb = gen2(shifted).view(shifted.size(0),-1)\n",
    "            for p in perturb:\n",
    "                offset_x.append(torch.round(p*8).int().numpy())\n",
    "            \n",
    "#torch.tensor(offset_x).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_x[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perturb_trace.log', 'w') as pfile:\n",
    "    for p in perturb_x[:100]:\n",
    "        for l in p:\n",
    "            pfile.write(str(l)+'\\n')\n",
    "with open('offset_trace.log', 'w') as pfile:\n",
    "    for p in offset_x[:100]:\n",
    "        for l in p:\n",
    "            pfile.write(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def capacity(oacc, zacc):\n",
    "    omarginal = 0.5*(oacc + 1 - zacc)\n",
    "    zmarginal = 0.5*(1+zacc-oacc)\n",
    "    psum = oacc*math.log2(oacc/omarginal)\n",
    "    psum += (1-oacc)*math.log2((1-oacc)/zmarginal)\n",
    "    psum += zacc*math.log2(zacc/zmarginal)\n",
    "    psum += (1-zacc)*math.log2((1-zacc)/omarginal)\n",
    "    return 0.5*psum\n",
    "capacity(0.52,0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "randloader = DataLoader(testset, batch_size=4, shuffle=True)\n",
    "loaditer = iter(randloader)\n",
    "x,y = next(loaditer)\n",
    "shifted = shifter(x)\n",
    "perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "original = x[:,31:]\n",
    "added = original+perturb.detach()\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "for idx in range(1,5):\n",
    "    ax1 = fig.add_subplot(1,4,idx)\n",
    "    ax1.set_title(str(y[idx-1].item()))\n",
    "    xaxis = np.arange(1,43)\n",
    "    yaxis = original[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis)\n",
    "    yaxis2 = added[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis2)\n",
    "    axes = plt.gca()\n",
    "    #print(perturb[idx-1]*8)\n",
    "    print(perturb[idx-1].mean().item()*testset.std)\n",
    "    #axes.set_ylim([145,220])\n",
    "plt.savefig('fig_perturb.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import GaussianGenerator, GaussianSinusoid, OffsetGenerator\n",
    "randloader = DataLoader(testset, batch_size=3, shuffle=True)\n",
    "loaditer = iter(randloader)\n",
    "x,y = next(loaditer)\n",
    "shifted = shifter(x)\n",
    "gau = nn.Sequential(GaussianGenerator(42, 10), nn.ReLU())\n",
    "sin = nn.Sequential(GaussianGenerator(42, 10), nn.ReLU())\n",
    "pad = OffsetGenerator(42, 5)\n",
    "gp = gau(shifted)\n",
    "sp = sin(shifted)\n",
    "pp = pad(shifted)\n",
    "perturb = pp\n",
    "pp[0] = gp[0]\n",
    "#pp[1] = sp[1]\n",
    "original = x[:,31:]\n",
    "added = original+perturb.detach()\n",
    "names = ['Gaussian', 'Constant Pad']\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "for idx in range(1,3):\n",
    "    ax1 = fig.add_subplot(1,2,idx)\n",
    "    ax1.set_title(names[idx-1])\n",
    "    xaxis = np.arange(1,43)\n",
    "    yaxis = original[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis)\n",
    "    yaxis2 = added[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis2)\n",
    "    axes = plt.gca()\n",
    "    #print(perturb[idx-1]*8)\n",
    "    print(perturb[idx-1].mean().item()*testset.std)\n",
    "    #axes.set_ylim([145,220])\n",
    "plt.savefig('fig_noise.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import RNNGenerator2\n",
    "cooldown = 100\n",
    "#halfstudent.eval()\n",
    "teacher = RNNGenerator2(42, scale=0.25, dim=128, drop=0.0).cuda()\n",
    "#teacher.load_state_dict(torch.load('./gans/best_adv_160.pth'))\n",
    "teacher.load_state_dict(torch.load('./models/best_adv_128.pth'))\n",
    "\n",
    "for _ in range(2):\n",
    "    classifier_test = CNNModelWide(42, dim=256).cuda()\n",
    "    #classifier_test = RNNModel(42, dim=256).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lastacc = 0.0\n",
    "    lastnorm = 0.0\n",
    "    optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "    for e in range(cooldown):\n",
    "        classifier_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata)\n",
    "            #train classifier\n",
    "            optim_c2.zero_grad()\n",
    "            perturb = teacher(shifted).view(shifted.size(0),-1)\n",
    "            #perturb = gen(xdata[:,31:])\n",
    "            #interleaving?\n",
    "            output = classifier_test(xdata[:,31:]+perturb.detach().float())\n",
    "            loss_c = criterion(output, ydata)\n",
    "            loss_c.backward()\n",
    "            optim_c2.step()\n",
    "\n",
    "\n",
    "        mloss = 0.0\n",
    "        totcorrect = 0\n",
    "        totcount = 0\n",
    "        mnorm = 0.0\n",
    "        zerocorrect = 0\n",
    "        zerocount = 0\n",
    "        onecorrect = 0\n",
    "        onecount = 0\n",
    "        #evaluate classifier\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier_test.eval()\n",
    "            for x,y in testloader:\n",
    "                xdata, ydata = x.cuda(), y.cuda()\n",
    "                shifted = shifter(xdata)\n",
    "                perturb = teacher(shifted).view(shifted.size(0),-1)\n",
    "                perturb = quantizer(perturb)\n",
    "                #perturb = gen(xdata[:,31:])\n",
    "                norm = torch.mean(perturb)\n",
    "                output = classifier_test(xdata[:,31:]+perturb.float())\n",
    "                loss_c = criterion(output, ydata)\n",
    "                pred = output.argmax(axis=-1)\n",
    "                mnorm += norm.item()/len(testloader)\n",
    "                mloss += loss_c.item()/len(testloader)\n",
    "                #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "                totcorrect += (pred==ydata).sum().item()\n",
    "                totcount += y.size(0)\n",
    "                zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "                zerocount += (ydata==0).sum().item()\n",
    "                onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "                onecount += (ydata==1).sum().item()\n",
    "            macc = float(totcorrect)/totcount\n",
    "            zacc = float(zerocorrect)/zerocount\n",
    "            oacc = float(onecorrect)/onecount\n",
    "            if (e+1)%10 == 0:\n",
    "                print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "            if cooldown - e <= 10:\n",
    "                lastacc += macc/10\n",
    "                lastnorm += mnorm/10\n",
    "            \n",
    "    print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "perturb_x = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = teacher(shifted.cuda()).view(shifted.size(0),-1)\n",
    "        perturbed_x = (x[:,31:]+perturb.cpu()).cpu()\n",
    "        for p in perturbed_x:\n",
    "            train_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            train_y.append(y_i.item())\n",
    "\n",
    "    for x,y in testloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = teacher(shifted.cuda()).view(shifted.size(0),-1)\n",
    "        perturbed_x = (x[:,31:]+perturb.cpu()).cpu()\n",
    "        for p in perturbed_x:\n",
    "            test_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            test_y.append(y_i.item())\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf2 = KNeighborsClassifier(n_neighbors=25)\n",
    "clf2.fit(train_x, train_y)\n",
    "pred_y = clf2.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
