{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RingDataset\n",
    "from Models import CNNModel, CNNModelWide, CNNModelDeep, RNNGenerator, Distiller, MLP, RNNModel, QGRU2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=256, num_workers=4)\n",
    "\n",
    "studentdim = 10\n",
    "gen=QGRU2(42, scale=0.25, dim=studentdim, drop=0.0)\n",
    "assert os.path.isfile('./models/best_{}_{}.pth'.format('qgru', studentdim))\n",
    "gen.load_state_dict(torch.load('./models/best_{}_{}.pth'.format('qgru', studentdim)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifter(arr, window=32):\n",
    "    dup = arr[:,None,:].expand(arr.size(0), arr.size(1)+1, arr.size(1))\n",
    "    dup2 = dup.reshape(arr.size(0), arr.size(1), arr.size(1)+1)\n",
    "    shifted = dup2[:,:window,:-window]\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen\n",
    "model.eval()\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.GRUCell, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "print(model_int8)\n",
    "\n",
    "input_fp32, _ = next(iter(testloader))\n",
    "shifted = shifter(input_fp32)\n",
    "\n",
    "'''\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "\n",
    "model_fp32_prepared(shifted, hidden_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "'''\n",
    "\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(shifted)\n",
    "res2 = model(shifted)\n",
    "print(res-res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quantizer(arr, std=8):\n",
    "    return torch.round(arr*std)/std\n",
    "\n",
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "valset =  RingDataset.RingDataset('core4ToSlice3_valid.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=128, num_workers=4)\n",
    "valloader = DataLoader(valset, batch_size=128, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "halfstudent = model_int8\n",
    "cooldown = 100\n",
    "#halfstudent.eval()\n",
    "for _ in range(3):\n",
    "    classifier_test = CNNModelWide(42, dim=256).cuda()\n",
    "    #classifier_test = RNNModel(42, dim=256).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lastacc = 0.0\n",
    "    lastnorm = 0.0\n",
    "    optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "    for e in range(cooldown):\n",
    "        classifier_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata.cpu())\n",
    "            #train classifier\n",
    "            optim_c2.zero_grad()\n",
    "            perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "            #perturb = gen(xdata[:,31:])\n",
    "            #interleaving?\n",
    "            output = classifier_test(xdata[:,31:]+perturb.detach().float())\n",
    "            loss_c = criterion(output, ydata)\n",
    "            loss_c.backward()\n",
    "            optim_c2.step()\n",
    "\n",
    "\n",
    "        mloss = 0.0\n",
    "        totcorrect = 0\n",
    "        totcount = 0\n",
    "        mnorm = 0.0\n",
    "        zerocorrect = 0\n",
    "        zerocount = 0\n",
    "        onecorrect = 0\n",
    "        onecount = 0\n",
    "        #evaluate classifier\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier_test.eval()\n",
    "            for x,y in testloader:\n",
    "                xdata, ydata = x.cuda(), y.cuda()\n",
    "                shifted = shifter(xdata.cpu())\n",
    "                perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "                perturb = quantizer(perturb)\n",
    "                #perturb = gen(xdata[:,31:])\n",
    "                norm = torch.mean(perturb)\n",
    "                output = classifier_test(xdata[:,31:]+perturb.float())\n",
    "                loss_c = criterion(output, ydata)\n",
    "                pred = output.argmax(axis=-1)\n",
    "                mnorm += norm.item()/len(testloader)\n",
    "                mloss += loss_c.item()/len(testloader)\n",
    "                #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "                totcorrect += (pred==ydata).sum().item()\n",
    "                totcount += y.size(0)\n",
    "                zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "                zerocount += (ydata==0).sum().item()\n",
    "                onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "                onecount += (ydata==1).sum().item()\n",
    "            macc = float(totcorrect)/totcount\n",
    "            zacc = float(zerocorrect)/zerocount\n",
    "            oacc = float(onecorrect)/onecount\n",
    "            if (e+1)%10 == 0:\n",
    "                print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "            if cooldown - e <= 10:\n",
    "                lastacc += macc/10\n",
    "                lastnorm += mnorm/10\n",
    "            \n",
    "    print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "        perturbed_x = x[:,31:]+perturb\n",
    "        for p in perturbed_x:\n",
    "            train_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            train_y.append(y_i.item())\n",
    "\n",
    "    for x,y in testloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "        perturbed_x = x[:,31:]+perturb\n",
    "        for p in perturbed_x:\n",
    "            test_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            test_y.append(y_i.item())\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=25)\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
