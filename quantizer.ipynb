{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RingDataset\n",
    "from Models import CNNModel, CNNModelWide, CNNModelDeep, RNNGenerator, Distiller, MLP, RNNModel, QGRU2\n",
    "import Models\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=256, num_workers=4)\n",
    "\n",
    "studentdim = 8\n",
    "gen=QGRU2(42, scale=0.25, dim=studentdim, drop=0.0)\n",
    "assert os.path.isfile('./gans/best_{}_{}.pth'.format('qgru', studentdim))\n",
    "gen.load_state_dict(torch.load('./gans/best_{}_{}.pth'.format('qgru', studentdim)))\n",
    "\n",
    "\n",
    "def quantizer(arr, std=8):\n",
    "    return torch.round(arr*std)/std\n",
    "\n",
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "valset =  RingDataset.RingDataset('core4ToSlice3_valid.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=128, num_workers=4)\n",
    "valloader = DataLoader(valset, batch_size=128, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifter(arr, window=32):\n",
    "    dup = arr[:,None,:].expand(arr.size(0), arr.size(1)+1, arr.size(1))\n",
    "    dup2 = dup.reshape(arr.size(0), arr.size(1), arr.size(1)+1)\n",
    "    shifted = dup2[:,:window,:-window]\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen\n",
    "model.eval()\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.GRUCell, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "print(model_int8)\n",
    "\n",
    "input_fp32, _ = next(iter(testloader))\n",
    "shifted = shifter(input_fp32)\n",
    "\n",
    "'''\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "\n",
    "model_fp32_prepared(shifted, hidden_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "'''\n",
    "\n",
    "halfstudent = model_int8\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "#res = model_int8(shifted)\n",
    "#res2 = model(shifted)\n",
    "#print(res-res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cooldown = 100\n",
    "#halfstudent.eval()\n",
    "for _ in range(3):\n",
    "    #classifier_test = CNNModel(42, dim=256).cuda()\n",
    "    classifier_test = MLP(42, dim=256).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lastacc = 0.0\n",
    "    lastnorm = 0.0\n",
    "    optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "    for e in range(cooldown):\n",
    "        classifier_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata.cpu())\n",
    "            #train classifier\n",
    "            optim_c2.zero_grad()\n",
    "            perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "            #perturb = gen(xdata[:,31:])\n",
    "            #interleaving?\n",
    "            output = classifier_test(xdata[:,31:]+perturb.detach().float())\n",
    "            loss_c = criterion(output, ydata)\n",
    "            loss_c.backward()\n",
    "            optim_c2.step()\n",
    "\n",
    "\n",
    "        mloss = 0.0\n",
    "        totcorrect = 0\n",
    "        totcount = 0\n",
    "        mnorm = 0.0\n",
    "        zerocorrect = 0\n",
    "        zerocount = 0\n",
    "        onecorrect = 0\n",
    "        onecount = 0\n",
    "        #evaluate classifier\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier_test.eval()\n",
    "            for x,y in testloader:\n",
    "                xdata, ydata = x.cuda(), y.cuda()\n",
    "                shifted = shifter(xdata.cpu())\n",
    "                perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "                perturb = quantizer(perturb)\n",
    "                #perturb = gen(xdata[:,31:])\n",
    "                norm = torch.mean(perturb)\n",
    "                output = classifier_test(xdata[:,31:]+perturb.float())\n",
    "                loss_c = criterion(output, ydata)\n",
    "                pred = output.argmax(axis=-1)\n",
    "                mnorm += norm.item()/len(testloader)\n",
    "                mloss += loss_c.item()/len(testloader)\n",
    "                #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "                totcorrect += (pred==ydata).sum().item()\n",
    "                totcount += y.size(0)\n",
    "                zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "                zerocount += (ydata==0).sum().item()\n",
    "                onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "                onecount += (ydata==1).sum().item()\n",
    "            macc = float(totcorrect)/totcount\n",
    "            zacc = float(zerocorrect)/zerocount\n",
    "            oacc = float(onecorrect)/onecount\n",
    "            if (e+1)%10 == 0:\n",
    "                print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "            if cooldown - e <= 10:\n",
    "                lastacc += macc/10\n",
    "                lastnorm += mnorm/10\n",
    "            \n",
    "    print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "perturb_x = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "        perturbed_x = x[:,31:]+perturb\n",
    "        for p in perturb:\n",
    "            perturb_x.append(torch.round(p*8).int().numpy())\n",
    "        for p in perturbed_x:\n",
    "            train_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            train_y.append(y_i.item())\n",
    "\n",
    "    for x,y in testloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "        perturbed_x = x[:,31:]+perturb\n",
    "        for p in perturbed_x:\n",
    "            test_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            test_y.append(y_i.item())\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf2 = KNeighborsClassifier(n_neighbors=25)\n",
    "clf2.fit(train_x, train_y)\n",
    "pred_y = clf2.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.support_vectors_.shape\n",
    "clf.dual_coef_.shape\n",
    "#clf.intercept_\n",
    "#p = clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in valloader:\n",
    "    shifted = shifter(x)\n",
    "    #train classifier\n",
    "    break\n",
    "\n",
    "traced_encoder = torch.jit.trace(halfstudent.encoder, shifted[0:1,:,0])\n",
    "hs = halfstudent.encoder(shifted[0:1,:,0])\n",
    "hs_h = torch.zeros_like(hs)\n",
    "traced_gru = torch.jit.trace(halfstudent.gru1, (hs, hs_h))\n",
    "print(traced_encoder.code)\n",
    "print(traced_gru.code)\n",
    "traced_decoder = torch.jit.trace(halfstudent.decoder, hs)\n",
    "print(traced_decoder.code)\n",
    "traced_encoder.save('quantized_encoder.pt')\n",
    "traced_gru.save('quantized_gru.pt')\n",
    "traced_decoder.save('quantized_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToScript(nn.Module):\n",
    "    def __init__(self, encoder, gru, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.gru = gru\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, h):\n",
    "        y = self.gru(self.encoder(x), h)\n",
    "        return self.decoder(y) , y\n",
    "toTrace = ToScript(halfstudent.encoder, halfstudent.gru1, halfstudent.decoder)\n",
    "traced_model = torch.jit.trace(toTrace, (torch.randn(1,32), torch.zeros(1,8)))\n",
    "traced_model.save('quantized_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_encoder(torch.rand(1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2 = Models.OffsetGenerator(42, scale=6.0)\n",
    "offset_x = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "            shifted = shifter(x)\n",
    "            #train classifier\n",
    "            perturb = gen2(shifted).view(shifted.size(0),-1)\n",
    "            for p in perturb:\n",
    "                offset_x.append(torch.round(p*8).int().numpy())\n",
    "            \n",
    "#torch.tensor(offset_x).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_x[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perturb_trace.log', 'w') as pfile:\n",
    "    for p in perturb_x[:100]:\n",
    "        for l in p:\n",
    "            pfile.write(str(l)+'\\n')\n",
    "with open('offset_trace.log', 'w') as pfile:\n",
    "    for p in offset_x[:100]:\n",
    "        for l in p:\n",
    "            pfile.write(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def capacity(oacc, zacc):\n",
    "    omarginal = 0.5*(oacc + 1 - zacc)\n",
    "    zmarginal = 0.5*(1+zacc-oacc)\n",
    "    psum = oacc*math.log2(oacc/omarginal)\n",
    "    psum += (1-oacc)*math.log2((1-oacc)/zmarginal)\n",
    "    psum += zacc*math.log2(zacc/zmarginal)\n",
    "    psum += (1-zacc)*math.log2((1-zacc)/omarginal)\n",
    "    return 0.5*psum\n",
    "capacity(0.52,0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "randloader = DataLoader(testset, batch_size=4, shuffle=True)\n",
    "loaditer = iter(randloader)\n",
    "x,y = next(loaditer)\n",
    "shifted = shifter(x)\n",
    "perturb = halfstudent(shifted).view(shifted.size(0),-1)\n",
    "original = x[:,31:]\n",
    "added = original+perturb.detach()\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "for idx in range(1,5):\n",
    "    ax1 = fig.add_subplot(1,4,idx)\n",
    "    ax1.set_title(str(y[idx-1].item()))\n",
    "    xaxis = np.arange(1,43)\n",
    "    yaxis = original[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis)\n",
    "    yaxis2 = added[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis2)\n",
    "    axes = plt.gca()\n",
    "    #print(perturb[idx-1]*8)\n",
    "    print(perturb[idx-1].mean().item()*testset.std)\n",
    "    #axes.set_ylim([145,220])\n",
    "plt.savefig('fig_perturb.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import GaussianGenerator, GaussianSinusoid, OffsetGenerator\n",
    "randloader = DataLoader(testset, batch_size=3, shuffle=True)\n",
    "loaditer = iter(randloader)\n",
    "x,y = next(loaditer)\n",
    "shifted = shifter(x)\n",
    "gau = nn.Sequential(GaussianGenerator(42, 10), nn.ReLU())\n",
    "sin = nn.Sequential(GaussianGenerator(42, 10), nn.ReLU())\n",
    "pad = OffsetGenerator(42, 5)\n",
    "gp = gau(shifted)\n",
    "sp = sin(shifted)\n",
    "pp = pad(shifted)\n",
    "perturb = pp\n",
    "pp[0] = gp[0]\n",
    "#pp[1] = sp[1]\n",
    "original = x[:,31:]\n",
    "added = original+perturb.detach()\n",
    "names = ['Gaussian', 'Constant Pad']\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "for idx in range(1,3):\n",
    "    ax1 = fig.add_subplot(1,2,idx)\n",
    "    ax1.set_title(names[idx-1])\n",
    "    xaxis = np.arange(1,43)\n",
    "    yaxis = original[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis)\n",
    "    yaxis2 = added[idx-1]*testset.std + testset.med\n",
    "    ax1.plot(xaxis,yaxis2)\n",
    "    axes = plt.gca()\n",
    "    #print(perturb[idx-1]*8)\n",
    "    print(perturb[idx-1].mean().item()*testset.std)\n",
    "    #axes.set_ylim([145,220])\n",
    "plt.savefig('fig_noise.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import RNNGenerator2\n",
    "cooldown = 100\n",
    "#halfstudent.eval()\n",
    "teacher = RNNGenerator2(42, scale=0.25, dim=128, drop=0.0).cuda()\n",
    "#teacher.load_state_dict(torch.load('./gans/best_adv_160.pth'))\n",
    "teacher.load_state_dict(torch.load('./models/best_adv_128.pth'))\n",
    "\n",
    "for _ in range(2):\n",
    "    classifier_test = CNNModelWide(42, dim=256).cuda()\n",
    "    #classifier_test = RNNModel(42, dim=256).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lastacc = 0.0\n",
    "    lastnorm = 0.0\n",
    "    optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "    for e in range(cooldown):\n",
    "        classifier_test.train()\n",
    "        for x,y in valloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata)\n",
    "            #train classifier\n",
    "            optim_c2.zero_grad()\n",
    "            perturb = teacher(shifted).view(shifted.size(0),-1)\n",
    "            #perturb = gen(xdata[:,31:])\n",
    "            #interleaving?\n",
    "            output = classifier_test(xdata[:,31:]+perturb.detach().float())\n",
    "            loss_c = criterion(output, ydata)\n",
    "            loss_c.backward()\n",
    "            optim_c2.step()\n",
    "\n",
    "\n",
    "        mloss = 0.0\n",
    "        totcorrect = 0\n",
    "        totcount = 0\n",
    "        mnorm = 0.0\n",
    "        zerocorrect = 0\n",
    "        zerocount = 0\n",
    "        onecorrect = 0\n",
    "        onecount = 0\n",
    "        #evaluate classifier\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier_test.eval()\n",
    "            for x,y in testloader:\n",
    "                xdata, ydata = x.cuda(), y.cuda()\n",
    "                shifted = shifter(xdata)\n",
    "                perturb = teacher(shifted).view(shifted.size(0),-1)\n",
    "                perturb = quantizer(perturb)\n",
    "                #perturb = gen(xdata[:,31:])\n",
    "                norm = torch.mean(perturb)\n",
    "                output = classifier_test(xdata[:,31:]+perturb.float())\n",
    "                loss_c = criterion(output, ydata)\n",
    "                pred = output.argmax(axis=-1)\n",
    "                mnorm += norm.item()/len(testloader)\n",
    "                mloss += loss_c.item()/len(testloader)\n",
    "                #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "                totcorrect += (pred==ydata).sum().item()\n",
    "                totcount += y.size(0)\n",
    "                zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "                zerocount += (ydata==0).sum().item()\n",
    "                onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "                onecount += (ydata==1).sum().item()\n",
    "            macc = float(totcorrect)/totcount\n",
    "            zacc = float(zerocorrect)/zerocount\n",
    "            oacc = float(onecorrect)/onecount\n",
    "            if (e+1)%10 == 0:\n",
    "                print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "            if cooldown - e <= 10:\n",
    "                lastacc += macc/10\n",
    "                lastnorm += mnorm/10\n",
    "            \n",
    "    print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "perturb_x = []\n",
    "with torch.no_grad():\n",
    "    for x,y in valloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = teacher(shifted.cuda()).view(shifted.size(0),-1)\n",
    "        perturbed_x = (x[:,31:]+perturb.cpu()).cpu()\n",
    "        for p in perturbed_x:\n",
    "            train_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            train_y.append(y_i.item())\n",
    "\n",
    "    for x,y in testloader:\n",
    "        shifted = shifter(x)\n",
    "        #train classifier\n",
    "        perturb = teacher(shifted.cuda()).view(shifted.size(0),-1)\n",
    "        perturbed_x = (x[:,31:]+perturb.cpu()).cpu()\n",
    "        for p in perturbed_x:\n",
    "            test_x.append(p.numpy())\n",
    "        for y_i in y:\n",
    "            test_y.append(y_i.item())\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf2 = KNeighborsClassifier(n_neighbors=25)\n",
    "clf2.fit(train_x, train_y)\n",
    "pred_y = clf2.predict(test_x)\n",
    "#tpred_y = clf.predict(train_x)\n",
    "(pred_y == test_y).sum()/len(pred_y)\n",
    "#(tpred_y == train_y).sum()/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
