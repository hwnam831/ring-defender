{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RingDataset\n",
    "from Models import CNNModel, RNNGenerator, Distiller, MLP, RNNModel, QGRU\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=256, num_workers=4)\n",
    "classifier_test = CNNModel(42, dim=256).cuda()\n",
    "studentdim = 32\n",
    "gen=QGRU(42, scale=0.25, dim=studentdim, drop=0.0)\n",
    "assert os.path.isfile('./models/best_{}_{}.pth'.format('qgru', studentdim))\n",
    "gen.load_state_dict(torch.load('./models/best_{}_{}.pth'.format('qgru', studentdim)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifter(arr, window=32):\n",
    "    dup = arr[:,None,:].expand(arr.size(0), arr.size(1)+1, arr.size(1))\n",
    "    dup2 = dup.reshape(arr.size(0), arr.size(1), arr.size(1)+1)\n",
    "    shifted = dup2[:,:window,:-window]\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNGen2(nn.Module):\n",
    "    def __init__(self, gen):\n",
    "        super().__init__()\n",
    "        self.encoder = gen.encoder\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.resblock = gen.resblock\n",
    "\n",
    "        self.decoder = gen.decoder\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(2, bsz, 16)\n",
    "                \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.quant(x)\n",
    "        encoded = self.encoder(x.permute(0,2,1)) #N,C,S -> N,S,C\n",
    "        hidden = self.quant(hidden)\n",
    "        res, hidden = self.resblock(encoded, hidden)\n",
    "        out = encoded + res #N,S,C\n",
    "        out = self.decoder(out).view(out.size(0),-1)\n",
    "        #out = out + self.scale*torch.randn_like(out)\n",
    "        #out = out + noise\n",
    "        \n",
    "        return self.dequant(torch.relu(out))\n",
    "gen2 = RNNGen2(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QGRU(\n",
      "  (encoder): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=32, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (gru1): DynamicQuantizedGRUCell(32, 32)\n",
      "  (gru2): DynamicQuantizedGRUCell(32, 32)\n",
      "  (decoder): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=32, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.1273,  0.0283,  0.0000,  ..., -0.0147,  0.0943, -0.1522],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.3346,  0.0323, -0.0957],\n",
      "        [-0.5370,  0.0000,  0.0000,  ..., -0.1543,  0.5100, -0.4143],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.9470, -0.3757,  0.4048],\n",
      "        [ 0.0000, -0.1592,  0.0000,  ...,  0.4305, -0.1351,  0.4698],\n",
      "        [-0.0582,  0.0000,  0.0000,  ...,  0.0000, -0.2967, -0.8013]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = gen\n",
    "model.eval()\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.GRUCell, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "print(model_int8)\n",
    "\n",
    "input_fp32, _ = next(iter(testloader))\n",
    "shifted = shifter(input_fp32)\n",
    "\n",
    "'''\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "\n",
    "model_fp32_prepared(shifted, hidden_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "'''\n",
    "\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(shifted)\n",
    "res2 = model(shifted)\n",
    "print(res-res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693106\t Avg perturb 1.681546\n",
      "\n",
      "epoch 2 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693088\t Avg perturb 1.682122\n",
      "\n",
      "epoch 3 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693102\t Avg perturb 1.682553\n",
      "\n",
      "epoch 4 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693103\t Avg perturb 1.682439\n",
      "\n",
      "epoch 5 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693083\t Avg perturb 1.683011\n",
      "\n",
      "epoch 6 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693082\t Avg perturb 1.681719\n",
      "\n",
      "epoch 7 \t zacc 0.000000\t oneacc 1.000000\t loss 0.693087\t Avg perturb 1.681907\n",
      "\n",
      "epoch 8 \t zacc 0.000827\t oneacc 0.998759\t loss 0.693056\t Avg perturb 1.682583\n",
      "\n",
      "epoch 9 \t zacc 0.004136\t oneacc 0.994210\t loss 0.693048\t Avg perturb 1.682250\n",
      "\n",
      "epoch 10 \t zacc 0.031431\t oneacc 0.978495\t loss 0.693018\t Avg perturb 1.681912\n",
      "\n",
      "epoch 11 \t zacc 0.114557\t oneacc 0.914392\t loss 0.692978\t Avg perturb 1.682365\n",
      "\n",
      "epoch 12 \t zacc 0.234905\t oneacc 0.792390\t loss 0.692896\t Avg perturb 1.682087\n",
      "\n",
      "epoch 13 \t zacc 0.403226\t oneacc 0.622002\t loss 0.692839\t Avg perturb 1.682976\n",
      "\n",
      "epoch 14 \t zacc 0.448304\t oneacc 0.583540\t loss 0.692709\t Avg perturb 1.682348\n",
      "\n",
      "epoch 15 \t zacc 0.453267\t oneacc 0.594706\t loss 0.692540\t Avg perturb 1.682247\n",
      "\n",
      "epoch 16 \t zacc 0.493383\t oneacc 0.541770\t loss 0.691972\t Avg perturb 1.683126\n",
      "\n",
      "epoch 17 \t zacc 0.433830\t oneacc 0.600496\t loss 0.692290\t Avg perturb 1.682425\n",
      "\n",
      "epoch 18 \t zacc 0.400744\t oneacc 0.644748\t loss 0.691267\t Avg perturb 1.682315\n",
      "\n",
      "epoch 19 \t zacc 0.337469\t oneacc 0.725806\t loss 0.690646\t Avg perturb 1.682461\n",
      "\n",
      "epoch 20 \t zacc 0.348222\t oneacc 0.712986\t loss 0.690972\t Avg perturb 1.682075\n",
      "\n",
      "epoch 21 \t zacc 0.316377\t oneacc 0.767990\t loss 0.687356\t Avg perturb 1.682562\n",
      "\n",
      "epoch 22 \t zacc 0.312655\t oneacc 0.769231\t loss 0.686729\t Avg perturb 1.682919\n",
      "\n",
      "epoch 23 \t zacc 0.273780\t oneacc 0.794045\t loss 0.689831\t Avg perturb 1.682203\n",
      "\n",
      "epoch 24 \t zacc 0.301902\t oneacc 0.782465\t loss 0.688002\t Avg perturb 1.683375\n",
      "\n",
      "epoch 25 \t zacc 0.318445\t oneacc 0.758892\t loss 0.690662\t Avg perturb 1.681640\n",
      "\n",
      "epoch 26 \t zacc 0.344086\t oneacc 0.749793\t loss 0.691412\t Avg perturb 1.683060\n",
      "\n",
      "epoch 27 \t zacc 0.401985\t oneacc 0.718776\t loss 0.690414\t Avg perturb 1.681371\n",
      "\n",
      "epoch 28 \t zacc 0.438792\t oneacc 0.688172\t loss 0.690179\t Avg perturb 1.682068\n",
      "\n",
      "epoch 29 \t zacc 0.469396\t oneacc 0.654260\t loss 0.693710\t Avg perturb 1.681666\n",
      "\n",
      "epoch 30 \t zacc 0.497105\t oneacc 0.643921\t loss 0.692316\t Avg perturb 1.682455\n",
      "\n",
      "Last 10 acc: 0.550103\t perturb: 1.682332\n"
     ]
    }
   ],
   "source": [
    "def quantizer(arr, std=8):\n",
    "    return torch.round(arr*std)/std\n",
    "\n",
    "testset =  RingDataset.RingDataset('core4ToSlice3_test.pkl', threshold=42)\n",
    "valset =  RingDataset.RingDataset('core4ToSlice3_valid.pkl', threshold=42)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=128, num_workers=4)\n",
    "valloader = DataLoader(valset, batch_size=128, num_workers=4)\n",
    "\n",
    "classifier_test = CNNModel(42, dim=256).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lastacc = 0.0\n",
    "lastnorm = 0.0\n",
    "optim_c2 = torch.optim.Adam(classifier_test.parameters(), lr=1e-4)\n",
    "\n",
    "halfstudent = model_int8\n",
    "cooldown = 30\n",
    "#halfstudent.eval()\n",
    "for e in range(cooldown):\n",
    "    classifier_test.train()\n",
    "    for x,y in valloader:\n",
    "        xdata, ydata = x.cuda(), y.cuda()\n",
    "        shifted = shifter(xdata.cpu())\n",
    "        #train classifier\n",
    "        optim_c2.zero_grad()\n",
    "        perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "        #perturb = gen(xdata[:,31:])\n",
    "        #interleaving?\n",
    "        output = classifier_test(xdata[:,31:]+perturb.detach().float())\n",
    "        loss_c = criterion(output, ydata)\n",
    "        loss_c.backward()\n",
    "        optim_c2.step()\n",
    "\n",
    "\n",
    "    mloss = 0.0\n",
    "    totcorrect = 0\n",
    "    totcount = 0\n",
    "    mnorm = 0.0\n",
    "    zerocorrect = 0\n",
    "    zerocount = 0\n",
    "    onecorrect = 0\n",
    "    onecount = 0\n",
    "    #evaluate classifier\n",
    "\n",
    "    with torch.no_grad():\n",
    "        classifier_test.eval()\n",
    "        for x,y in testloader:\n",
    "            xdata, ydata = x.cuda(), y.cuda()\n",
    "            shifted = shifter(xdata.cpu())\n",
    "            perturb = halfstudent(shifted).view(shifted.size(0),-1).cuda()\n",
    "            perturb = quantizer(perturb)\n",
    "            #perturb = gen(xdata[:,31:])\n",
    "            norm = torch.mean(perturb)\n",
    "            output = classifier_test(xdata[:,31:]+perturb.float())\n",
    "            loss_c = criterion(output, ydata)\n",
    "            pred = output.argmax(axis=-1)\n",
    "            mnorm += norm.item()/len(testloader)\n",
    "            mloss += loss_c.item()/len(testloader)\n",
    "            #macc += ((pred==ydata).sum().float()/pred.nelement()).item()/len(testloader)\n",
    "            totcorrect += (pred==ydata).sum().item()\n",
    "            totcount += y.size(0)\n",
    "            zerocorrect += ((pred==0)*(ydata==0)).sum().item()\n",
    "            zerocount += (ydata==0).sum().item()\n",
    "            onecorrect += ((pred==1)*(ydata==1)).sum().item()\n",
    "            onecount += (ydata==1).sum().item()\n",
    "        macc = float(totcorrect)/totcount\n",
    "        zacc = float(zerocorrect)/zerocount\n",
    "        oacc = float(onecorrect)/onecount\n",
    "        print(\"epoch {} \\t zacc {:.6f}\\t oneacc {:.6f}\\t loss {:.6f}\\t Avg perturb {:.6f}\\n\".format(e+1, zacc, oacc, mloss, mnorm))\n",
    "        if cooldown - e <= 10:\n",
    "            lastacc += macc/10\n",
    "            lastnorm += mnorm/10\n",
    "print(\"Last 10 acc: {:.6f}\\t perturb: {:.6f}\".format(lastacc,lastnorm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
